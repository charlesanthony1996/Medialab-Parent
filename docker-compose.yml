services:
  frontend:
    build: ./AI_Frontend
    ports:
      - "127.0.0.1:80:80"
    restart: on-failure
    depends_on:
      - server
      - openai_backend
    networks:
      - backend_network
    volumes:
      - './dist:/dist'
    env_file:
      - './.env'

  # ollama:
  #   image: ollama/ollama
  #   ports:
  #     - "127.0.0.1:4001:11434"
  #   networks:
  #     - backend_network
  #   restart: on-failure

  server:
    build: ./AI_Server
    ports:
      - "127.0.0.1:8000:8000"
    volumes:
      - ./AI_Server:/app
    env_file:
      - './.env'
    restart: on-failure
    networks:
      - backend_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  filter:
    build: ./AI_Filter
    ports:
      - "127.0.0.1:7001:7001"
    env_file:
      - './.env'
    restart: on-failure
    networks:
      - backend_network

  openai_backend:
    build: ./AI_API-Wrapper
    ports:
      - "127.0.0.1:6001:6001"
    env_file:
      - './.env'
    restart: on-failure
    networks:
      - backend_network

  database:
    build: ./AI_database
    ports:
      - "127.0.0.1:5001:5001"
    env_file:
      - './.env'


networks:
  backend_network:
    driver: bridge
